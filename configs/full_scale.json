{
  "description": "Production-scale configuration with all optimizations - ~12-24 hours total",
  "estimated_time": "12-24 hours",
  "use_case": "Full-scale training for competitive-level checkers AI with maximum optimization",
  
  "game_config": {
    "row_count": 8,
    "col_count": 8,
    "buffer_count": 2,
    "draw_move_limit": 50,
    "repetition_limit": 3,
    "history_timesteps": 8
  },
  
  "model_config": {
    "num_resBlocks": 12,
    "num_hidden": 128,
    "use_checkpoint": true
  },
  
  "training_args": {
    "C": 1.5,
    "num_searches": 200,
    "num_iterations": 100,
    "num_self_play_iterations": 100,
    "num_epochs": 20,
    "batch_size": 64,
    "initial_temperature": 1.5,
    "final_temperature": 0.6,
    "use_temperature": true,
    "dirichlet_epsilon": 0.15,
    "dirichlet_alpha": 0.3,
    "lr_decay_steps": 20,
    "lr_decay_factor": 0.75
  },
  
  "optimizer_config": {
    "lr": 0.001,
    "weight_decay": 0.0001
  },
  
  "use_mixed_precision": true,
  
  "notes": [
    "High MCTS simulations (800) for maximum game quality",
    "Large network (12 blocks, 128 hidden) for complex pattern recognition",
    "Extended history timesteps (8) for deeper positional understanding",
    "Long training schedule (100 iterations) for convergence",
    "Many self-play games (100 per iteration) for diverse training data",
    "Conservative learning rate with gradual decay for stability",
    "Mixed precision essential for memory efficiency with large network",
    "Gradient checkpointing crucial for training 12-block network on single GPU",
    "Dynamic temperature schedule optimized for long training",
    "Lower Dirichlet noise for more focused exploration in production",
    "Automatic checkpointing critical for long training sessions",
    "Expected to produce master-level checkers play"
  ]
}